%\documentclass{beamer}
\documentclass[nojss]{jss}

%\VignetteIndexEntry{Estimation of the multinomial logit models in  R: The \[mhr\]logit Packages}
%\VignetteDepends{Formula, statmod}
%\VignetteKeywords{discrete choice models, maximum likelihood estimation, R, econometrics}
%\VignettePackage{mlogit}

\author{Yves Croissant\\Universit\'e de la R\'eunion}

\Plainauthor{Yves Croissant}

\title{Estimation of multinomial logit models in \proglang{R} : The \pkg{mlogit} Packages}

\Plaintitle{Estimation of multinomial logit models in R :
  The mlogit Packages}



\Keywords{discrete choice models, maximum likelihood estimation,
  \proglang{R}, econometrics}

\Plainkeywords{discrete choice models, maximum likelihood estimation,
  R, econometrics}



\Abstract{ \pkg{mlogit} is a package for \proglang{R} which enables
  the estimation the multinomial logit models with individual and/or
  alternative specific variables. The main extensions of the basic
  multinomial model (heteroscedastic, nested and random parameter
  models) are implemented.}


\Address{
Yves Croissant\\
LET-ISH\\
Avenue Berthelot\\
F-69363 Lyon cedex 07\\
Telephone: +33/4/78727249 \\
Fax: +33/4/78727248 \\
E-mail: \email{yves.croissant@let.ish-lyon.cnrs.fr}
\\
}

%% need no \usepackage{Sweave.sty}




\begin{document}

\SweaveOpts{engine=R,eps=FALSE}

%\maketitle

\section*{An introductory example}


The logit model is useful when one tries to explain discrete choices,
\emph{i.e.} choices of one among several mutually exclusive
alternatives. There are many useful applications in different fields
of applied econometrics when one wants to analyze individual data,
which may be :

\begin{itemize}
\item revealed preferences data which means that the data are observed
  choices of individual for example for a transport mode (car, plane
  and train for example),
\item stated preferences data, for example three virtual train tickets
  with different characteristics proposed to travelers
  \begin{itemize}
  \item A : a train ticket which costs 10 euros, for a trip of 30
    minutes and one change,
  \item B : a train ticket which costs 20 euros, for a trip of 20
    minutes and no change,
  \item C : a train ticket which costs 22 euros, for a trip of 22
    minutes and one change.
  \end{itemize}
\end{itemize}


Suppose that the utility of each alternative depends linearly on cost
($x$) and price ($z$)

$$
\left\{
\begin{array}{rcl}
U_1&=&\alpha_1+\beta x_1+ \gamma z_1\\
U_2&=&\alpha_2+\beta x_2+ \gamma z_2 \\
U_3&=&\alpha_3+\beta x_3+ \gamma z_3\\
\end{array}
\right.
$$

The multinomial logit model is obtained simply by applying a specific
transformation to the utility level so that the results may be
interpreted as probabilities of choosing each alternative :

$$
\left\{
\begin{array}{rcl}
\mbox{P}_1&=&\frac{e^{U_1}}{e^{U_1}+e^{U_2}+e^{U_3}} \\
\mbox{P}_2&=&\frac{e^{U_2}}{e^{U_1}+e^{U_2}+e^{U_3}} \\
\mbox{P}_3&=&\frac{e^{U_3}}{e^{U_1}+e^{U_2}+e^{U_3}} \\
\end{array}
\right.
$$

The two characteristics of probabilities are satisfied :

\begin{itemize}
\item $0 \leq \mbox{P}_j \leq 1$,
\item $\sum_{j=1}^{3} \mbox{P}_j =1$
\end{itemize}


Once fitted, a logit model is useful for predictions :
\begin{itemize}
\item enter new values for the explanatory variables, 
\item get 
  \begin{itemize}
  \item at an individual level the probabilities of choice,
  \item at an aggregate level the market shares.
  \end{itemize}
\end{itemize}


Consider, as an example interurban trips between two towns (Lyon and
Paris for example). Suppose that there are three modes (car, plane and
train) and that the characteristics of the modes and the market shares
are as follow :

\begin{center}
\begin{tabular}{lccc} \hline
 & price & time & share\\ \hline 
car & 50 & 4 & 20\%\\
plane & 150 & 1 & 25\%\\
train & 80 & 2 &55\%\\ \hline
\end{tabular}
\end{center}

With a sample of travelers, on can estimate the coefficients of the
logit model, \emph{i.e.} the coefficients of time and price in the
utility function.

The fitted model can then be used to predict the impact of some chocks
on the market shares, for example :

\begin{itemize}
\item the influence of train trips length on modal shares,
\item the influence of the arrival of low cost companies.
\end{itemize}

To get the predictions, one just has to change the values of train
time or plane prices and compute the new probabilities, which can be
interpreted at the aggregate level as predicted market shares.


\section{Data management and model description}

\subsection{Data management}


<<echo=FALSE,results=hide>>=
options(prompt= "R> ", useFancyQuotes = FALSE)
@

\proglang{mlogit} is loaded using :

<<echo=TRUE, results=hide>>=
library("mlogit")
@ 

It comes with several data sets that we'll use to illustrate the
features of the library. Data sets used for multinomial logit
estimation deals with some individuals, that make one or several
choices between several alternatives, the determinants of these
choices being variables that can be alternative specific or purely
individual specific. Such data have therefore a specific structure
which can be characterized by three indexes : 

\begin{itemize}
\item the alternative,
\item the choice situation,
\item the individual
\end{itemize}

the last one being only relevant if we have repeated observations for
the same individual.

Data sets can have two different shapes :

\begin{itemize}
\item a \emph{wide} shape : in this case, there is one row for each
  choice situation,
\item a \emph{long} shape : in this case, there is one column for
  each alternative.
\end{itemize}

This can be illustrated with two data sets. The first one,
\code{Fishing} comes with \proglang{mlogit}. The second one
\code{TravelMode} is from the \proglang{AER} package.

<<>>=
data("Fishing", package = "mlogit")
head(Fishing, 3)
@ 

There are four fishing modes (beach, pier, boat, charter), two
alternative specific variables (price and catch) and one
choice/individual specific variable (income)\footnote{Note that the
  distinction between choice and individual is not relevant here as
  these data are not panel data.}. This ``wide'' format is suitable to
store individual specific variable. Otherwise, it is cumbersome for
alternative specific variables because there are as many columns for
such variables that there are alternatives. 

<<>>=
data("TravelMode", package="AER")
head(TravelMode)
@ 

There are four transport modes (air, train, bus and car)and most of
the variable are alternative specific (wait, vcost, travel,
gcost). The only individual specific variables are income and
size. This advantage of this shape is that there are much fewer
columns than in the wide format, the caveat being that values of
income and size are repeated four times.

\proglang{mlogit} deals with both format. It provides a
\code{mlogit.data} function that take as first argument a
\code{data.frame} and returns a \code{data.frame} in ``long'' format
with some information about the structure of the data. 

For the \code{Fishing} data, we would use :

<<>>=
Fish <- mlogit.data(Fishing, shape="wide", varying=2:9, choice="mode")
@ 

The mandatory arguments are \code{choice}, which is the variable that
indicates the choice made, the shape of the original \code{data.frame}
and, if there are some alternative specific variables, \code{varying}
which is a numeric vector that indicates which columns contains
alternative specific variables. This argument is then passed to
\code{reshape} that coerced the original data.frame in ``long''
format. Further arguments may be passed to \code{reshape}. For
example, if the names of the variables are of the form \code{var:alt},
one can add \code{sep = ':'}.

<<>>=
head(Fish, 5)
head(attr(Fish, "index"), 5)
@ 

The result is a \code{data.frame} in ``long format'' with one line for
each alternative. The ``choice'' variable is now a boolean and the
individual specific variable (income) is repeated 4 times. An
\code{index} attribute is added to the data, which contains the two
relevant index : \code{chid} is the choice index and \code{alt} index.

For data in ``long'' format like \code{TravelMode}, the \code{shape}
(here equal to \code{long}) and the \code{choice} arguments are still
mandatory. 

The information about the structure of the data can be explicitly
indicated or, in part, guessed by the \code{mlogit.data}
function. Here, we have 210 individuals which are indicated by a
variable called \code{individual}. The information about individuals
can also be guessed from the fact that the data frame is balanced
(every individual faces 4 alternatives) and that the rows are ordered
first by individual and then by alternative.

Concerning the alternative, there are indicated by the \code{mode}
variable and they can also be guessed tanks to the ordering and the
rows and the fact that the data frame is balanced.

The first way to read correctly this data frame is to ignore
completely the two index variables. In this case, the only
supplementary argument to provide a \code{alt.levels} argument which
is a character vector that contains the name of the alternatives :

<<>>=
TM <- mlogit.data(TravelMode,
choice = "choice", shape = "long", alt.levels = c("air", "train",
"bus", "car"))
@ 

It is also possible to provide an argument \code{alt.var} which
indicates the name of the variable that contains the alternatives

<<>>=
TM <- mlogit.data(TravelMode ,choice = "choice", shape = "long",
                  alt.var = "mode")
@ 

The name of the variable that contains the information about the
choice can be indicated using the \code{chid.var} variable :

<<>>=
TM <- mlogit.data(TravelMode, choice = "choice",
                  shape = "long", chid.var = "individual",
                  alt.levels = c("air", "train", "bus", "car"))
@ 

Both alternative and choice variable can be provided :

<<>>=
TM <- mlogit.data(TravelMode, choice = "choice", shape = "long",
                 chid.var = "individual", alt.var = "mode")
@ 

and dropped from the data using the \code{drop.index} argument : 

<<>>=
TM <- mlogit.data(TravelMode, choice = "choice", shape = "long",
                 chid.var = "individual", alt.var = "mode", drop.index = TRUE)
head(TM)
@ 

The final example is a data set called \code{Train} which contains
data from a stated preference study. 

<<>>=
data("Train", package="mlogit")
head(Train, 3)
@ 

These data are panel data, each individual has responded to several
(up to 16) scenario. To take this panel dimension into account, one
has to add an argument \code{id} which contains the individual
variable. The \code{index} attribute has now a supplementary column,
the individual index.

<<>>=
Tr <- mlogit.data(Train, shape = 'wide', choice="choice", 
                  varying=4:11, sep="", alt.levels=c(1, 2), id = "id")
head(Tr, 3)
head(attr(Tr, "index"), 3)
@ 

\subsection{Model description}

\code{mlogit} use the standard \code{formula, data} interface to
describe the model to be estimated. However, standard \code{formula}s
are not very practical for such models. More precisely, when working
with multinomial logit models, one has to consider three kinds of
variables :

\begin{itemize}
\item alternative specific variables $x_{ij}$ with a generic
  coefficient $\beta$,
\item individual specific variables $z_i$ with alternative specific
  coefficients $\gamma_j$,
\item alternative specific variables $w_{ij}$ with an alternative
  specific coefficient $\delta_j$.
\end{itemize}

The utility for the alternative $j$ (or more precisely the
deterministic component of utility) is then :

$$
U_{ij}=\alpha_j + \beta x_{ij} + \gamma_j z_i + \delta_j w_{ij}
$$

Utility being ordinal, only utility differences are relevant to
modelize the choice for one alternative. This means that, for example,
we'll be interested in the difference between the utility of two
different alternatives $j$ and $k$ :

$$
U_{ij}-U_{ik}=(\alpha_j-\alpha_k) + \beta (x_{ij}-x_{ik}) + 
(\gamma_j-\gamma_k) z_i + (\delta_j w_{ij} - \delta_k w_{ik})
$$

It is clear from the previous expression that coefficients for
individual specific variables (the intercept being one of those)
should be alternative specific, otherwise they would disappear in the
differentiation. Moreover, only differences of these coefficients are
relevant and may be identified. For example, with three alternatives
1, 2 and 3, the three coefficients $\gamma_1, \gamma_2, \gamma_3$
associated to an individual specific variable cannot be identified, but
only two linear combinations of them. Therefore, one has to make a
choice of normalization and the most simple one is just to put
$\gamma_1 = 0$.

Coefficients for alternative specific variables may (or may not) be
alternative specific. For example, transport time is alternative
specific, but may be 10 mn in public transport don't have the same
value than 10 mn in a car. In this case, alternative specific
coefficients are relevant. Monetary time is also alternative specific,
but in this case, one can consider than 1 euro is 1 euro whatever it
is spent in car or in public transports. In this case a generic
coefficient is relevant.

A model with only individual specific variables is sometimes called a
\emph{multinomial logit model}, one with only alternative specific
variables a \emph{conditional logit model} and one with both kind of
variables a \emph{mixed logit model}. This is seriously misleading :
\emph{conditional logit model} is also a logit model for longitudinal
data in the statistical literature and \emph{mixed logit} is one of
the names of a logit model with random parameters. Therefore, in what
follow, we'll use the name \emph{multinomial logit model} for the
model we've just described whatever the kind of variables introduced.

\code{mlogit} package provides objects of class \code{mFormula} which
are extended model formulas and which are build upon \code{Formula}
objects provided by the \code{Formula} package.

To illustrate the use of \code{mFormula} objects, let's use again the
\code{TravelMode} data set. \code{income} and \code{size} (the size of
the household) are individual specific variables. \code{vcost}
(monetary cost) and \code{travel} (travel time) are alternative
specific. We want to use a generic coefficient for the former and
alternative specific coefficients for the latter. This is done using
the following three-parts formula :

<<>>=
f <- mFormula(choice ~ vcost | income + size | travel)
@ 

By default, an intercept is added to the model, it can be removed by
using \code{+0} or \code{-1} in the second part. Some parts may be
omitted when there are no ambiguity. For example, the following couples
of \code{formula}s are identical :

<<>>=
f2 <- mFormula(choice ~ vcost + travel | income + size)
f2 <- mFormula(choice ~ vcost + travel | income + size | 0)
@ 

<<>>=
f3 <- mFormula(choice ~ 0 | income | 0)
f3 <- mFormula(choice ~ 0 | income)
@ 

<<>>=
f4 <- mFormula(choice ~ vcost + travel)
f4 <- mFormula(choice ~ vcost + travel | 1)
f4 <- mFormula(choice ~ vcost + travel | 1 | 0)
@ 

Finally, we show below some \code{formula}s that describe models
without intercepts (which is generally hardly relevant)

<<>>=
f5 <- mFormula(choice ~ vcost | 0 | travel)
f6 <- mFormula(choice ~ vcost | income + 0 | travel)
f6 <- mFormula(choice ~ vcost | income -1 | travel)
f7 <- mFormula(choice ~ 0 | income -1 | travel)
@ 

\code{model.matrix} and \code{model.frame} methods are provided for
\code{mFormula} objects. The former is of particular interest, as
illustrated in the following example :

<<>>=
f <- mFormula(choice ~ vcost | income  | travel)
head(model.matrix(f, TM))
@ 

The model matrix contains $J-1$ columns for every individual specific
variable (\code{income} and the intercept), which means that the
coefficient associated to the first alternative (\code{air}) is fixed
to 0.

It contains only one column for \code{vcost} because we want a generic
coefficient for this variable. 

It contains $J$ columns for \code{travel}, because it is an
alternative specific variable for which we want an alternative
specific coefficient.


\section{Random utility model and the multinomial logit model}

\subsection{Random utility model}

The individual must choose one alternative among J different and
exclusive alternatives. A level of utility may be defined for each
alternative and the individual is supposed to choose the alternative
with the highest level of utility. Utility is supposed to be the sum
of two components\footnote{when possible, we'll omit the individual
  index to simplify the notations.}:

\begin{itemize}
\item a systematic component, denoted $V_j$, which is a function of
  different observed variables $x_j$. For sake of simplicity, it will
  be supposed that this component is a linear function of the observed
  explanatory variables : $V_j = \beta_j^\top x_j$,
\item an unobserved component $\epsilon_j$ which, from the researcher
  point of view, can be represented as a random variable. This error
  term include the impact of all the unobserved variables which have
  an impact on the utility of choosing a specific alternative.
\end{itemize}

It is very important to understand that the utility and therefore the
choice is purely deterministic from the individual point of view. It
is random form the researcher's point of view, because some of the
determinants of the utility are unobserved, which implies that the
choice can only be analyzed in terms of probabilities.


We have, for each alternative, the following utility levels :

$$
\left\{
\begin{array}{rclcl}
U_1&=&\beta_1^\top x_1+\epsilon_1&=&V_1+\epsilon_1\\
U_2&=& \beta_1^\top x_1+\epsilon_2&=&V_2+\epsilon_2\\
 & \vdots &  & \vdots &  \\
U_J&=&\beta_J^\top x_J+\epsilon_J&=&V_J+\epsilon_J\\
\end{array}
\right.
$$

alternative $l$ will be chosen if and only if $\forall \;\; j \neq l
\;\; U_j > U_l$ which leads to the following $J-1$ conditions :

$$
\left\{
\begin{array}{rcl}
U_l-U_1&=&(V_l-V_1)+(\epsilon_l-\epsilon_1)>0\\
U_l-U_2&=&(V_l-V_2)+(\epsilon_l-\epsilon_2)>0\\
 & \vdots &  \\
U_l-U_J&=&(V_l-V_J)+(\epsilon_l-\epsilon_J)>0\\
\end{array}
\right.
$$


As $\epsilon_j$ are not observed, choices can only be modeled in
terms of probabilities from the researcher point of view. The $J-1$
conditions can be rewritten in terms of upper bonds for the $J-1$
remaining error terms :

$$
\left\{
\begin{array}{rcl}
\epsilon_1&<&(V_l-V_1)+\epsilon_l\\
\epsilon_2&<&(V_l-V_2)+\epsilon_l\\
 & \vdots &  \\
\epsilon_J&<&(V_l-V_J)+\epsilon_l\\
\end{array}
\right.
$$

The general expression of the probability of choosing alternative $l$
is then :

$$
(\mbox{P}_l \mid \epsilon_l)=\mbox{P}(U_l>U_1,\ldots,U_l>U_J)
$$

\begin{equation}
  \label{eq:condprobgen}
  (\mbox{P}_l \mid \epsilon_l)=
  F_{-l}(\epsilon_1<(V_l-V_1)+\epsilon_l, \ldots, \epsilon_J<(V_l-V_J)+\epsilon_l)
\end{equation}

where $F_{-l}$ is the multivariate distribution of $J-1$ error terms (all
the $\epsilon$'s except $\epsilon_l$). Note that this probability is
conditional on the value of $\epsilon_l$.

The unconditional probability (which depends only on $\beta$ and on
the value of the observed explanatory variables is :

$$
  \mbox{P}_l=\int(\mbox{P}_l \mid \epsilon_l)f_l(\epsilon_l)
  d\epsilon_l
$$

\begin{equation}
  \label{eq:uncondprobgen}
  \mbox{P}_l=\int F_{-l}((V_l-V_1)+\epsilon_l, \ldots,(V_l-V_J)+\epsilon_l)f_l(\epsilon_l) d\epsilon_l
\end{equation}

where $f_l$ is the marginal density function of $\epsilon_l$.

\subsection{The distribution of the error terms}

The multinomial logit model (\cite{MCFAD:74}) is a special case of the
model developed in the previous section. It relies on three
hypothesis :

\textbf{H1 : independence of errors}

If the hypothesis of independence of errors is made, we have :

$$
\left\{
\begin{array}{rcl}
\mbox{P}(U_l>U_1)&=&F_1(V_l-V_1+\epsilon_l)\\
\mbox{P}(U_l>U_2)&=&F_2(V_l-V_2+\epsilon_l)\\
 & \vdots &  \\
\mbox{P}(U_l>U_J)&=&F_J(V_l-V_J+\epsilon_l)\\
\end{array}
\right.
$$

And the conditional (\ref{eq:condprobgen}) and unconditional
(\ref{eq:uncondprobgen}) probabilities are just :


\begin{equation}
  \label{eq:probcondind}
  (\mbox{P}_l \mid \epsilon_l)=\prod_{j\neq l}F_j(V_l-V_j+\epsilon_l)
\end{equation}


\begin{equation}
  \label{eq:probuncondind}
  \mbox{P}_l =\int \prod_{j\neq l}F_j(V_l-V_j+\epsilon_l) \; f_l(\epsilon_l) \;d\epsilon_l
\end{equation}

which means that the evaluation of only one-dimensional integral is
required to compute the probabilities.

\textbf{H2 : Gumbel distribution}

Each $\epsilon$ follows a \textsc{Gumbel} distribution :

$$
f(z)=\frac{1}{\theta}e^{\frac{\mu-z}{\theta}} e^{-e^{\frac{\mu-z}{\theta}}}
$$

where $\mu$ is the location parameter and $\theta$ the scale parameter.

$$
P(z<t)=F(t)=\int_{-\infty}^t \frac{1}{\theta}e^{\frac{\mu-z}{\theta}}
e^{-e^{\frac{\mu-z}{\theta}}} dz=e^{-e^{-\frac{t}{\theta}}}
$$

The first two moments of the \textsc{Gumbel} distribution are
$\mbox{E}(z)=\mu+\theta \gamma$, where $\gamma$ is the
Euler-Mascheroni constant (0.577) and
$\mbox{V}(z)=\frac{\pi^2}{6}\theta^2$.

The mean and the variance of the $\epsilon_j$s are not identified. We
can then, without loss of generality suppose that $\mu_j=0 \;\;
\forall j$ and that one of the $\theta_j$ equals 1.

$$
U_l=\beta_l^\top x_l+\eta_l
$$

$$
\frac{U_l}{\sigma}=\frac{\beta_l}{\sigma}^\top
x_l+\frac{\eta_l}{\sigma} =\frac{\beta_l}{\sigma}^\top x_l+\epsilon_l
$$

with $\epsilon_l=\frac{\eta_l}{\sigma}$ follows a standard Gumbel
distribution


\textbf{H3 identically distributed errors}

As, the location is not identified for any error term, this hypothesis
is essentially an homoscedasticity hypothesis, which means that the
scale parameter of \textsc{Gumbel} distribution is the same for all
the alternatives. This common scale parameter is not identified, and
therefore, we can suppose that $\theta_j = 1\;\forall j \in 1\ldots
J$.

In this case, the conditional (\ref{eq:probcondind}) and unconditional
(\ref{eq:probuncondind}) probabilities further simplify to :

\begin{equation}
  \label{eq:probcondml}
  (\mbox{P}_l \mid \epsilon_l)=\prod_{j\neq l}F(V_l-V_j+\epsilon_l)
\end{equation}


\begin{equation}
  \label{eq:probuncondml}
  \mbox{P}_l =\int \prod_{j\neq l}F(V_l-V_j+\epsilon_l) \; f(\epsilon_l) \;d\epsilon_l
\end{equation}

with $F$ and $f$ respectively the cumulative and the density of the
standard \textsc{Gumbel} distribution (\emph{i.e.} with position and
scale parameters equal to 0 and 1).

\subsection{Computation of the logit probabilities}

With these hypothesis on the distribution of the error terms, we can
now show that the probabilities have very simple, closed forms, which
correspond to the logit transformation of the deterministic parts of
the utility. 

Let's start with the probability that the alternative $l$ is better
than one other alternative $j$. With hypothesis 2 and 3, it can be
written :

\begin{equation}
  \label{eq:preflj}
  P(\epsilon_j<V_l-V_j+\epsilon_l)=e^{-e^{-(V_l-V_j+\epsilon_l)}}
\end{equation}

With hypothesis 1, the probability of choosing $l$ is then simply the
product of probabilities (\ref{eq:preflj}) for all the alternatives
except $l$ :

\begin{equation}
  \label{eq:condprobml}
  (P_l \mid \epsilon_l) =\prod_{j\neq l}e^{-e^{-(V_l-V_j+\epsilon_l)}}
\end{equation}

The unconditional probability is the mean of the previous expression
weighted by the Gumbell density of $\epsilon_l$.


\begin{equation}
  \label{eq:condprobml}
  P_l=\int_{-\infty}^{+\infty}\left(P_l \mid
    \epsilon_l\right)e^{-\epsilon_l}e^{-e^{-\epsilon_l}} d\epsilon_l
  =\int_{-\infty}^{+\infty}\left( \prod_{j\neq l}e^{-e^{-(V_i-V_j+\epsilon_l)}}  \right)e^{-\epsilon_l}e^{-e^{-\epsilon_l}} d\epsilon_l
\end{equation}

We first begin by writing the preceding expression for \emph{all}
alternatives, including the $l$ alternative.

$$
P_l=\int_{-\infty}^{+\infty}\left(
  \prod_{j}e^{-e^{-(V_l-V_j+\epsilon_l)}} \right)e^{-\epsilon_l}
d\epsilon_l
$$

$$
P_l=\int_{-\infty}^{+\infty} e^{-\sum_j
  e^{-(V_l-V_j+\epsilon_l)}}e^{-\epsilon_l} d\epsilon_l
=\int_{-\infty}^{+\infty} e^{-e^{-\epsilon_l} \sum_j
  e^{-(V_i-V_j)}}e^{-\epsilon_l} d\epsilon_l
$$

We then use the following change of variable

$$
t=e^{-\epsilon_l} \Rightarrow dt=-e^{-\epsilon_l}d\epsilon_l 
$$

The unconditional probability is therefore the following integral :

$$
P_l=-\int_{0}^{+\infty} e^{-t \sum_j e^{-(V_l-V_j)}}dt
$$

which has a closed form :

$$
P_l=-\left[ \frac{e^{-t \sum_j e^{-(V_l-V_j)}}}{\sum_j e^{-(V_l-V_j)}}\right]_{0}^{+\infty}
=\frac{1}{\sum_j e^{-(V_l-V_j)}}
$$

and can be rewritten as the usual logit probability :

\begin{equation}
  \label{eq:finprobml}
  P_l=\frac{e^{V_i}}{\sum_j e^{V_j}}
\end{equation}

% \subsection{Multinomial vs conditional logit models}

% Two kind of explanatory variables may enter the utility function :
% $x_{ij}$ are alternative specific variable (price, time of a transport
% mode for example) and $z_i$ are individual specific variable (income,
% gender) :


% The utility level of individual $i$ for the alternative $l$ is :

% $$
% U_{il}=\alpha_l+\beta x_{il}+\gamma_l z_i+\epsilon_{il}
% $$

% The utility difference between two alternatives $l$ and $j$ is then :

% $$
% U_{il}-U_{ij}=(\alpha_l-\alpha_j)+\beta (x_{il}-x_{ij})+(\gamma_l-\gamma_j) z_i
% +\epsilon_{il}-\epsilon_{ij}
% $$

% It is obvious from this utility difference that $\alpha$ and $\gamma$
% \emph{must} be alternative specific. Conversely, $\beta$ may be the
% same for all alternative.

% The level of utility is not relevant, only utility differences
% matter. Therefore, only $J-1$ $\alpha$ and $\gamma$ out of $J$ may be
% estimated. Any normalization rule may be applied. For example, one can
% impose that $\alpha_1=\gamma_1=0$.


\subsection{IIA hypothesis}

If we consider the probabilities of choice for two alternatives $l$
and $m$, we have :

$$
P_l=\frac{e^{V_l}}{\sum_j e^{V_j}}
$$

$$
P_m=\frac{e^{V_m}}{\sum_j e^{V_j}}
$$

The ration of these two probabilities is :

$$
\frac{P_l}{P_m}=\frac{e^{V_l}}{e^{V_m}}
$$

This probability ratio for the two alternatives depends only on the
characteristics of these two alternatives and not on those of other
alternatives. This is called the \textsc{IIA} hypothesis (for
independence of irrelevant alternatives).

If we use again the introductory example of urban trips between Lyon
and Paris :

\begin{center}
\begin{tabular}{lccc} \hline
 & price & time & share\\ \hline 
car & 50 & 4 & 20\%\\
plane & 150 & 1 & 20\%\\
train & 80 & 2 &60\%\\ \hline
\end{tabular}
\end{center}

Suppose that, because of low cost companies arrival, the price of
plane is now 100\$. The market share of plane will increase (for
example up to 60\%). With a logit model, share for train / share for
car is 3 before the price change, and will remain the same after the
price change. Therefore, the new predicted probabilities for car and
train are 10 and 30\%.

The \emph{IIA} hypothesis relies on the hypothesis of independence of the
error terms. It is not a problem by itself and may even be considered
as a useful feature for a well specified model. However, this
hypothesis may be in practice violated if some important variables are
unobserved.

To see that, suppose that the utilities for two alternatives are :

$$U_{i1}=\alpha_1+\beta_1 z_i+\gamma x_{i1}+\epsilon_{i1}$$
$$U_{i2}=\alpha_2+\beta_2 z_i+\gamma x_{i2}+\epsilon_{i2}$$

with $\epsilon_{i1}$ and $\epsilon_{i2}$ uncorrelated. In this case,
the logit model can be safely used, as the hypothesis of independence
of the errors is satisfied.

If $z_i$ is unobserved, the estimated model is :

$$U_{i1}=\alpha_1+\gamma x_{i1}+\eta_{i1}$$
$$U_{i2}=\alpha_2+\gamma x_{i2}+\eta_{i2}$$
$$\eta_{i1}=\epsilon_{i1}+\beta_1 z_i$$
$$\eta_{i2}=\epsilon_{i2}+\beta_2 z_i$$

The error terms are now correlated because part of them is the common
influence of some omitted variables on utility.


% \subsubsection{IIA test}

% Suppose we can form some nests, for example ground modes (bus, car,
% train) and air modes (air) and that IIA may applies only within nests.

% Consider a model estimated on the subset of ground modes and a model
% estimated for all modes :

% \begin{itemize}
% \item If IIA applies globally, both models are consistent and the
%   model estimated for all the modes is more efficient,
% \item if IIA applies only within nests, the model estimated on all the
%   modes is unconsistent. 
% \end{itemize}

% This is the base of the Hausman McFadden IIA test. 

\subsection{Estimation}

The coefficients of the multinomial logit model are estimated using
maximum likelihood. 

\subsubsection{The likelihood function}

Let's start with a very simple example. Suppose there are four
individuals. For given parameters and explanatory variables, we can
calculate the probabilities. The likelihood for the sample is the
probability associated to the sample :

\begin{center}
\begin{tabular}{lccccc}\hline
  & choice & $\mbox{P}_{i1}$ & $\mbox{P}_{i2}$ & $\mbox{P}_{i3}$ & $l_i$ \\ \hline
  1 & 1 & 0.5 & 0.2 & 0.3 & 0.5 \\
  2 & 3 & 0.2 & 0.4 & 0.4 & 0.4 \\
  3 & 2 & 0.6 & 0.1 & 0.3 & 0.1 \\
  4 & 2 & 0.3 & 0.6 & 0.1 & 0.6 \\\hline
\end{tabular}
\end{center}

With random sample the joint probability for the sample is simply the
product of the probabilities associated with every observation.
 
$$\mbox{L}= 0.5 \times 0.4 \times 0.1 \times 0.6$$

$y_{ij}$ is equal to one if individual $i$ made choice $j$, 0
otherwise.

The probability of the choice made for one individual is :

$$
\mbox{P}_i=\prod_j \mbox{P}_{ij}^{y_{ij}}
$$

Or in log :

$$
\ln \mbox{P}_i=\sum_j y_{ij} \ln \mbox{P}_{ij}
$$

which leads to the log-likelihood function :

$$
\ln L = \sum_i \ln \mbox{P}_i=\sum_i \sum_j y_{ij} \ln \mbox{P}_{ij}
$$


\subsubsection{Numerical optimization}

We seek to calculate the maximum of a function $f$.

\begin{enumerate}
\item Start with a value $\beta_t$, 
\item Approximate the function to optimize by a second order
  \textsc{Taylor} series : $l(x)=f(\beta_t)+(x-\beta_t)g(\beta_t)+0.5
  (x-\beta_t)^2h(\beta_t)$ where $g$ and $h$ are the first two
  derivatives of $f$, 
\item find the maximum of $l(x)$. The first order condition is :
$\frac{\partial{l(x)}}{\partial{x}}=g(\beta_t)+
(x-\beta_t)h(\beta_t)=0$. The solution is : $x=\beta_t-\frac{g(\beta_t)}{h(\beta_t)}$ 
\item Go back to step one with that value.
\end{enumerate}

  \begin{center}
    \begin{figure}
      \includegraphics[width=.75\textwidth]{./graph/opt1}
      \caption{Numerical optimization}
    \end{figure}
  \end{center}

  \begin{center}
    \begin{figure} 
      \includegraphics[width=.75\textwidth]{./graph/opt2}
      \caption{Numerical optimization}
    \end{figure}
  \end{center}

  \begin{center}
    \begin{figure}
    \includegraphics[width=.75\textwidth]{./graph/opt3}
    \caption{Numerical optimization}
    \end{figure}
  \end{center}


Consider now a function of several variables $f(\beta)$. The vector of
first derivatives (called the gradient) is denoted $g$ and the matrix of
second derivatives (called the hessian) is denoted $H$. The second order
approximation is :

$$l(x)=f(\beta_t)+(x-\beta_t)g(\beta_t)+0.5 (x-\beta_t)'H(\beta_t)(x-\beta_t)$$

The vector of first derivatives is :

$$\frac{\partial l(x)}{\partial x}=g(\beta_t)+H(\beta_t)(x-\beta_t)$$

$$
x=\beta_t-H(\beta_t)^{-1}g(\beta_t)
$$



Two kinds of routines are currently used for maximum likelihood
estimation. The first one can be called ``Newton-like'' methods. In
this case, at each iteration, an estimation of the hessian is
calculated, whether using the second derivatives of the function
(Newton-Ralphson method) or using the outer product of the gradient
(BHHH). This approach is very powerful if the function is
well-behaved, but it may performs poorly otherwise and scratch after a
few iterations.

The second one, BFGS, updates at each iteration the estimation of the
hessian. It is often more robust and may performs well in cases where
the first one doesn't work.

Two optimization functions are included in core \proglang{R}:
\code{nlm} which use the Newton-Ralphson method and \code{optim} which
use BFGS (among other methods). Recently, the \pkg{maxLik} package
\citep{MAXLIK/08} provides a unified approach. With a unique
interface, all the previously described methods are available.

The behavior of \code{maxLik} can be controlled by the user using in
the estimation function arguments like \texttt{print.level} (from
0-silent to 2-verbal), \texttt{iterlim} (the maximum number of
iterations), \texttt{methods} (the method used, one of \texttt{"nr"},
\texttt{"bhhh"} or \texttt{"bfgs"}) that are passed to \code{maxLik}.



\subsubsection{Gradient and Hessian for the logit model}


$$
\frac{\partial \ln P_{ij}}{\partial \beta}=x_{ij}-\sum_l P_{il}x_{il}
$$

$$
\frac{\partial \ln L}{\partial \beta}=\sum_i \sum_j \left(y_{ij}- P_{ij}\right)x_{ij}
$$

$$
\frac{\partial^2 \ln L}{\partial \beta\partial \beta'}=\sum_i \sum_j P_{ij}\left(x_{ij}-\sum_l P_{il}x_{il}\right)
\left(x_{ij}-\sum_l P_{il}x_{il}\right)'
$$

\subsection{Interpretation}

\subsubsection{Marginal effects}

The coefficients are not directly interpretable. The marginal effects
are obtained by deriving the probabilities with respect with the
variables :

$$
\frac{\partial P_{ij}}{\partial z_{i}}=P_{ij}\left(\beta_j-\sum_l P_{il}\beta_l\right)
$$

$$
\frac{\partial P_{ij}}{\partial x_{ij}}=\gamma P_{ij}(1-P_{ij})
$$

$$
\frac{\partial P_{ij}}{\partial x_{il}}=-\gamma P_{ij}P_{il}
$$

\begin{itemize}
\item   For a choice specific variable, the sign of the coefficient is
  directly interpretable. The product of two probabilities is at most
  0.25.
\item For an individual specific variable, the sign of the coefficient
  is not necessarily the sign of the coefficient. Actually, it depends
  on the sign of $\left(\beta_j-\sum_l P_{il}\beta_l\right)$, which
  would be positive if the coefficient for the $j$ alternative is
  greater than a weighted average of the coefficients for all the
  alternative, the weights being the probabilities of choosing the
  alternatives.
\end{itemize}

\subsubsection{Marginal rates of substitution}

Coefficients are marginal utilities, which are not interpretable
because utility is ordinal. However, ratios of coefficients are
marginal rates of substitution, which are interpretable. For example,
if the observable part of utility is : $V=\beta_o +\beta_1 x_1 +\beta
x_2 + \beta x_3$ ; join variations of $x_1$ and $x_2$ which ensure the
same level of utility are such that : $dV=\beta_1 dx_1+\beta_2 dx_2=0$
so that :

$$
- \frac{dx_2}{dx_1}\mid_{dV = 0} = \frac{\beta_1}{\beta_2}
$$


For example, if $x_2$ is transport cost (in euros), $x_1$ transport
time (in hours), $\beta_1 = 1.5$ and $\beta_2=0.2$,
$\frac{\beta_1}{\beta_2}=30$ is the marginal rate of substitution of
time in terms of euros, the value of 30 means that to reduce the
travel time of one hour, the individual is willing to pay at most 30
euros more.


\subsubsection{Consumer's surplus}

The level of utility attained by an individual is
$U_j=V_j+\epsilon_j$, $j$ being the alternative chosen. The expected
utility, from the researcher's point of view is then :

$$
\mbox{E}(\max_j U_j)
$$

where the expectation is taken on the values of all the error
terms. If the marginal utility of income ($\alpha$) is known and
constant, the expected surplus is simply $\mbox{E}(max_j U_j)/\alpha$.

This expected surplus is a very simple expression in the context of
the logit model, which is called the ``sum log''. We'll demonstrate
this fact in the context of two alternatives.

With two alternatives, the values of $\epsilon_1$ and $\epsilon_2$ can
be depicted in a plan. Within this plan, some points corresponds to
situations where alternative 1 is chosen and some where alternative 2
is chosen. More precisely, alternative 1 is chosen if $\epsilon_2 \leq
V_1-V_2 + \epsilon_1$ and alternative 2 is chosen if $\epsilon_1 \leq
V_2-V_1 + \epsilon_2$. The first expression is the equation of a
straight line in the plan which delimits the choice for the two
alternatives.

We can then write the expected utility as the sum of two terms $E_1$
and $E_2$, with :

$$
E_1= \int_{\epsilon_1=-\infty}^{\infty}\int_{-\infty}^{V_1 - V_2 +
  \epsilon_1}(V_1+\epsilon_1)f(\epsilon_1)f(\epsilon_2)d\epsilon_1
d\epsilon_2
$$


and 


$$
E_2= \int_{\epsilon_2=-\infty}^{\infty}\int_{-\infty}^{V_2 - V_1 +
  \epsilon_1}(V_2+\epsilon_2)f(\epsilon_1)f(\epsilon_2)d\epsilon_1
d\epsilon_2
$$

with $f(z)=exp(-e^(-z))$ the density of the Gumbell distribution.

$$
E_1=
\int_{\epsilon_1=-\infty}^{\infty}(V_1+\epsilon_1)\left(\int_{-\infty}^{V_1
    - V_2 +
    \epsilon_1}f(\epsilon_2)d\epsilon_2\right)f(\epsilon_1)d\epsilon_1
$$

The expression in brackets is the cumulative density of
$\epsilon_2$. We then have :

$$
E_1=
\int_{\epsilon_1=-\infty}^{\infty}(V_1+\epsilon_1)e^{-e^{-(V_1-V_2)-\epsilon_1}}f(\epsilon_1)d\epsilon_1
$$

  
$$
E_1=
\int_{\epsilon_1=-\infty}^{\infty}(V_1+\epsilon_1)e^{-\epsilon_1}e^{-ae^{-\epsilon_1}}f(\epsilon_1)d\epsilon_1
$$

with
$a=1+e^{-(V_1-V_2)}=\frac{e^{V_1}+e^{V_2}}{e^{V_1}}=\frac{1}{P_1}$

Let define $z \mid e^{-z}=ae^{-\epsilon_1} \Leftrightarrow z =
\epsilon_1 - \ln a$

We then have :

$$
E_1=
\int_{\epsilon_1=-\infty}^{\infty}(V_1+z+\ln a)/ae^{-z}e^{-e^{-z}}f(z)dz
$$

$$
E_1= (V1+\ln a)/a+\mu /a
$$

$$
E_1= \frac{\ln(e^{V_1}+e^{V_2}) + \mu}{(e^{V_1}+e^{V_2})/e^{V_1}}
=\frac{e^{V_1}\ln(e^{V_1}+e^{V_2}) + e^{V_1}\mu}{e^{V_1}+e^{V_2}}
$$

By symmetry, 

$$
E_2= \frac{e^{V_2}\ln(e^{V_1}+e^{V_2}) + e^{V_2}\mu}{e^{V_1}+e^{V_2}}
$$

And then :

$$
\mbox{E}(U)=E_1+E_2= \ln (e^{V_1}+e^{V_2})+\mu
$$


More generally, in presence of $J$ alternatives, we have :

$$
\mbox{E}(U)=\ln \sum_{j=1}^Je^{V_j}+\mu
$$

and the expected surplus is, with $\alpha$ the constant marginal
utility of income~:

$$
\mbox{E}(U)=\frac{\ln \sum_{j=1}^Je^{V_j}+\mu}{\alpha}
$$



\subsection{Application}

\code{Train} contains data about a stated preference survey in
Netherlands. Users are asked to choose between to train trips
characterized by four attributes :

\begin{itemize}
\item price : the price in cents of guilders,
\item time : travel time in minutes,
\item change : the number of changes,
\item comfort : the class of comfort, 0, 1 or 2, 0 being the most
  comfortable class.
\end{itemize}

<<>>=
data("Train", package="mlogit")
Tr <- mlogit.data(Train, shape = 'wide', choice="choice", 
                  varying=4:11, sep="", alt.levels=c(1, 2), id = "id")

@ 

We first convert \code{price} and \code{time} in more meaningful
unities, hours and euros (1 guilder is $2.20371$ euros) :

<<>>=
Tr$price <- Tr$price / 100 * 2.20371
Tr$time <- Tr$time / 60
@ 

We then estimate the model : both alternatives being virtual train
trips, it is relevant to use only generic coefficients and to remove
the intercept :

<<>>=
m <- mlogit(choice~price+time+change+comfort | -1, Tr)
summary(m)
@ 

All the coefficients are highly significant and have the predicted
negative sign (remind than an increase in the variable \code{comfort}
implies using a less comfortable class). The coefficients are not
directly interpretable, but dividing them by the price coefficient, we
get monetary values :

<<>>=
coef(m)[-1]/coef(m)[1]
@ 

We obtain the value of 26 euros for an hour of traveling, 5 euros for
a change and 14 euros to access a more comfortable class. 

The second example use the \code{Fishing} data. It illustrates the
multi-part formula interface to describe the model, and the fact that
it is not necessary to transform the data set using \code{mlogit.data}
before the estimation, \emph{i.e.} instead of using :

<<>>=
Fish <- mlogit.data(Fishing, shape="wide", varying=2:9, choice="mode")
m <- mlogit(mode~price | income | catch, Fish)
@ 

it is possible to use \code{mlogit} with the original
\code{data.frame} and the relevant arguments that will be internally
passed to \code{mlogit.data} :

<<>>=
m <- mlogit(mode~price | income | catch, Fishing, shape = "wide", varying = 2:9)
summary(m)
@

Several methods can be used to extract some results from the estimated
model. \code{fitted} returns the predicted probabilities for the
outcome or for all the alternatives if \code{outcome=FALSE}.

<<>>=
head(fitted(m))
head(fitted(m, outcome=FALSE))
@ 

Finally, two further arguments can be usefully used while using
\proglang{mlogit}

\begin{itemize}
\item \code{reflevel} indicates which alternative is the ``reference''
  alternative, \emph{i.e.} the one for which the coefficients are 0,
\item \code{altsubset} indicates a subset on which the estimation has
  to be performed ; in this case, only the lines that corresponds to
  the selected alternatives are used and all the observations which
  corresponds to choices for unselected alternatives are removed : 
\end{itemize}  
  
  
<<>>=
m <- mlogit(mode~price | income | catch, Fish, reflevel='charter', alt.subset=c('beach', 'pier', 'charter'))
@ 

\section{Relaxing the iid hypothesis}

With hypothesis 1 and 3, the error terms are \emph{iid} (identically
and independently distributed), \emph{i.e.} not correlated and
homoscedastic. Extensions of the basic multinomial logit model have
been proposed by relaxing one of these two hypothesis while
maintaining the second hypothesis of \textsc{Gumbell} distribution. 

\subsection{The heteroskedastic logit model}

The heteroskedastic logit model was proposed by \cite{BHAT:95}.

The probability that $U_l>U_j$ is :

$$
P(\epsilon_j<V_l-V_j+\epsilon_l)=e^{-e^{-\frac{(V_l-V_j+\epsilon_l)}{\theta_j}}}
$$

which implies the following conditional and unconditional
probabilities

\begin{equation}
  \label{eq:condprobh}
  (P_l \mid \epsilon_l) =\prod_{j\neq
    l}e^{-e^{-\frac{(V_l-V_j+\epsilon_l)}{\theta_j}}}
\end{equation}


\begin{equation}
  \label{eq:uncondprobh}
  P_l=\int_{-\infty}^{+\infty} \prod_{j\neq l}
  \left(e^{-e^{-\frac{(V_l-V_j+\epsilon_l)}{\theta_j}}}\right)\frac{1}{\theta_l}e^{-\frac{\epsilon_l}{\theta_l}}e^{-e^{-\frac{\epsilon_l}{\theta_l}}}
  d\epsilon_l
\end{equation}


We then apply the following change of variable :

$$
u =e^{-\frac{\epsilon_l}{\theta_l}}\;\Rightarrow\; du =
-\frac{1}{\theta_l}e^{-\frac{\epsilon_l}{\theta_l}}d\epsilon_l
$$

The unconditional probability (\ref{eq:uncondprobh}) can then be rewritten :

$$
P_l = \int_{0}^{+\infty}\prod_{j \neq
  l}\left(e^{-e^{-\frac{V_l-V_j-\theta_l \ln
        u}{\theta_j}}}\right)e^{-u}du =
\int_{0}^{+\infty}\left(e^{-\sum_{j \neq l}e^{-\frac{V_l-V_j-\theta_l
        \ln u}{\theta_j}}}\right)e^{-u}du
$$

There is no closed form for this integral but it can be written the
following way :


$$
P_l=\int_{0}^{+\infty}G_le^{-u} du
$$

with

$$
G_l=e^{-A_l}\;\;\;A_l=\sum_{j\neq
  l}\alpha_{j}\;\;\;\alpha_{j}=e^{-\frac{V_l-V_j-\theta_l\ln
    u}{\theta_j}}
$$

This one-dimensional integral can be efficiently computed using a Gauss
quadrature method, and more precisely a Gauss-Laguerre quadrature method :

$$
\int_0^{+\infty}f(u)e^{-u}du=\sum_t f(u_t) w_t
$$

where $u_t$ and $w_t$ are respectively the nodes and the weights.

$$
P_l=\sum_t G_l(u_t) w_t
$$

$$
\frac{\partial G_l}{\partial \beta_k}=\sum_{j\neq l}\frac{\alpha_{j}}{\theta_j}(x_{lk}-x_{jk})G_l
$$

$$
\frac{\partial G_l}{\partial \theta_l}=-\ln u \sum_{j\neq l}\frac{\alpha_{j}}{\theta_j}G_l
$$


$$
\frac{\partial G_l}{\partial \theta_j}=\ln \alpha_{j}\frac{\alpha_{j}}{\theta_j}G_l
$$

\subsection{The nested logit model}


The nested logit model was first proposed by \cite{MCFAD:78}. It is a
generalization of the multinomial logit model that rests on the idea
that some alternatives may be joined in several groups (called
nests). The error terms may then present some correlation in the same
nest, whereas error terms of different nests are still uncorrelated.

We suppose that the alternatives can be put into $K$ different
nests. This implies the following multivariate distribution for the
error terms.

$$
\mbox{exp}\left(-\sum_{k=1}^K \left( \sum_{j \in B_k}
    e^{-\epsilon_j/\lambda_k}\right)^{\lambda_k}\right)
$$

The marginal distributions of the $\epsilon$s are still univariate
extreme value, but there is now some correlation within
nests. $1-\lambda_k$ is a measure of the correlation, \emph{i.e.}
$\lambda_k = 1$ implies no correlation. It can then be shown that the
probability of choosing alternative $j$ that is part of nest $l$ is :

$$
P_j = \frac{e^{V_j/\lambda_l}\left(\sum_{j \in B_l}
    e^{V_j/\lambda_l}\right)^{\lambda_l-1}} {\sum_{k=1}^K\left(\sum_{j
      \in B_k} e^{V_j/\lambda_k}\right)^{\lambda_k}}
$$

Let write : $V_j=Z_j+W_l$


$$
P_j=\frac{e^{(Z_j+W_l)/\lambda_l}}{\sum_{j \in B_l}
    e^{(Z_j+W_l)/\lambda_l}}\times 
\frac{\left(\sum_{j \in B_l} e^{(Z_j+W_l)/\lambda_l}\right)^{\lambda_l}}
{\sum_{k=1}^K\left(\sum_{j
      \in B_k} e^{(Z_j+W_k)/\lambda_k}\right)^{\lambda_k}}
$$

$$
P_j=\frac{e^{Z_j/\lambda_l}}{\sum_{j \in B_l}
    e^{Z_j/\lambda_l}}\times 
\frac{\left(\sum_{j \in B_l} e^{(Z_j+W_l)/\lambda_l}\right)^{\lambda_l}}
{\sum_{k=1}^K\left(\sum_{j
      \in B_k} e^{(Z_j+W_k)/\lambda_k}\right)^{\lambda_k}}
$$


$$
\left(\sum_{j \in B_l} e^{(Z_j+W_l)/\lambda_l}\right)^{\lambda_l}
= \left(e^{W_l/\lambda_l}\sum_{j \in B_l} e^{Z_j/\lambda_l}\right)^{\lambda_l}
=e^{W_l+\lambda_l I_l}
$$

with $I_l=\ln \sum_{j \in B_l} e^{Z_j/\lambda_l}$ wich is often
denoted as the inclusive value or inclusive utility. 


We then can write the probability of choosing alternative $j$ as :

$$
P_j=\frac{e^{Z_j/\lambda_l}}{\sum_{j \in B_l}
    e^{Z_j/\lambda_l}}\times 
\frac{e^{W_l+\lambda_l I_l}}{\sum_{k=1}^Ke^{W_k+\lambda_k I_k}}
$$


The first term $\mbox{P}_{j\mid l}$ is the conditional probability of
choosing alternative $j$ if nest $l$ is chosen. It is often referred as
the \emph{lower model}. The second term $\mbox{P}_l$ is the marginal
probability of choosing the nest $l$ and is referred as the \emph{upper
  model}. 

$W_k+\lambda_k I_k$ can be interpreted as the expected utility of
choosing the best alternative of the nest $k$, $W_k$ being the
expected utility of choosing an alternative in this nest (whatever
this alternative is) and $\lambda_k I_k$ being the expected extra
utility he receives by being able to choose the best alternative in
the nest.

The inclusive values link the two models. 

It is then straightforward to show that IIA applies within nests, but
not for two alternatives in different nests.

A slightly different version of the nested logit model is often used,
but is not compatible with the random utility maximization
hypothesis. Its difference with the previous expression is that the
determinist parts of the utility for each alternative is not
normalized by the nest elasticity :

$$
P_j = \frac{e^{V_j}\left(\sum_{j \in B_l}
    e^{V_j}\right)^{\lambda_l-1}} {\sum_{k=1}^K\left(\sum_{j \in B_k}
    e^{V_j}\right)^{\lambda_k}}
$$

The gradient is, for the first version of the model and denoting $A_j
= e^{V_k/\lambda_k}$ and $N_k = \sum_{j \in B_k}A_j$ :

$$
\left\{
\begin{array}{rcl}
  \frac{\partial \ln P_j}{\partial \beta}&=&\frac{x_j}{\lambda_k} +
  \frac{\lambda_k-1}{\lambda_k}\frac{1}{N_k}\sum_{j \in B_k}A_jx_j -
  \frac{1}{\sum_k N_k^{\lambda_k}} \sum_k N_k ^{\lambda_k - 1}\sum_{j
    \in B_k} A_j x_j\\
  \frac{\partial \ln P_j}{\partial \lambda_k}&=&-\frac{V_j}{\lambda_k^2}+\ln N_k -\frac{\lambda_k-1}{\lambda_k^2}\frac{1}{N_k}\sum_{j \in B_k} V_jA_j \\
  &&- \frac{1}{\sum_k N_k^{\lambda_k}}\left(\ln N_k-\frac{1}{\lambda_kN_k}\sum_{j \in B_k}V_jA_j\right)
\end{array}
\right.
$$


For the unscaled version, $A_l= e^{V_l}$ and the gradient is :

$$
\left\{
\begin{array}{rcl}
  \frac{\partial \ln P_j}{\partial \beta}&=&A_j x_j +
  (\lambda_l-1)\frac{1}{N_l}\sum_{j \in B_l}A_jx_j - \frac{1}{\sum_k
    N_k^{\lambda_k}} \sum_k \lambda_k N_k ^{\lambda_k - 1}\sum_{j \in B_k}A_j
  x_j\\
  \frac{\partial \ln P_j}{\partial \lambda_l}&=&\ln N_l - \frac{1}{\sum_k N_k^{\lambda_k}}N_l^{\lambda_l}\ln N_l
\end{array}
\right.
$$


To illustrate the estimation of nested logit models, we use an
application presented by Kenneth Train. The data consists on 250 newly
built houses in California, and we seek to explain the heating system
chosen. The data is available in \code{mlogit} under the name
\code{HC}. Seven heating modes are available :


\begin{description}
\item[gcc] gas central heat with cooling,
\item[ecc] electric central resistance heat with cooling,
\item[erc] electric room resistance heat with cooling,
\item[hpc] electric heat pump which provides cooling also,
\item[gc] gaz central heat without cooling,
\item[ec] electric central resistance heat without cooling,
\item[er] electric room resistance heat without cooling.
\end{description}

The covariates are the installation cost (\code{ich}), the operating
cost (\code{och}) and the income of the household. This data set has a
natural nesting structure, the first four modes providing also cooling
whereas the three other modes being ``pure'' heating modes. For the
cooling mode, the installation and operating cost for the cooling part
(\code{icca} and \code{occa} should be added.

<<>>=
data("HC", package = "mlogit")
HC <- mlogit.data(HC, varying = c(2:8, 10:16), choice = "depvar", shape = "wide")
cooling.modes <- attr(HC, "index")$alt %in% c('gcc', 'ecc', 'erc', 'hpc')
room.modes <- attr(HC, "index")$alt %in% c('erc', 'er')
HC$icca[!cooling.modes] <- 0
HC$occa[!cooling.modes] <- 0
HC$icca <- HC$icca / 100
HC$occa <- HC$occa / 100
HC$ich <- HC$ich / 100
HC$och <- HC$och / 100


HC$inc.cooling <- HC$inc.room <- 0
HC$inc.cooling[cooling.modes] <- HC$income[cooling.modes]
HC$inc.room[room.modes] <- HC$income[room.modes]
HC$int.cooling <- as.numeric(cooling.modes)
nl <- mlogit(depvar ~ ich + och +icca + occa + inc.room + inc.cooling + int.cooling | 0, HC,
             nests = list(cooling = c('gcc','ecc','erc','hpc'), 
             other = c('gc', 'ec', 'er')), un.nest.el = TRUE)
@ 


% The nested logit model was first proposed by \cite{MCFAD:78}.

% Suppose that the alternatives can be put into $K$ different nests.
% The error terms has the following cumulative distribution :

% $$
% \mbox{exp}\left(-\sum_{k=1}^K \left( \sum_{j \in B_k}
%     e^{-\epsilon_j/\lambda_k}\right)^{\lambda_k}\right)
% $$

% The marginal distributions of the $\epsilon$s are still univariate
% extreme value, but there is now some correlation within
% nests. $1-\lambda_k$ is a measure of the correlation, \emph{i.e.}
% $\lambda_k = 1$ implies no correlation. It can then be shown that the
% probability of choosing alternative $j$ is :

% $$
% P_j = \frac{e^{V_j/\lambda_k}\left(\sum_{j \in B_k} e^{V_j/\lambda_k}\right)^{\lambda_k-1}}
% {\sum_{k=1}^K\left(\sum_{j \in B_k} e^{V_j/\lambda_k}\right)^{\lambda_k}}
% $$

% It is then straightforward to show that IIA applies within nests, but
% not for two alternatives in different nests.

% A slightly different version of the nested logit model is often used,
% but is not compatible with the random utility maximisation
% hypothesis. Its difference with the previous expression is that the
% determinist parts of the utility for each alternative is not
% normalized by the nest elasticity :

% $$
% P_j = \frac{e^{V_j}\left(\sum_{j \in B_k} e^{V_j}\right)^{\lambda_k-1}}
% {\sum_{k=1}^K\left(\sum_{j \in B_k} e^{V_j}\right)^{\lambda_k}}
% $$

% The gradient is, for the first version of the model and denoting $A_j
% = e^{V_j/\lambda_k}$ and $N_k = \sum_{j \in B_k}A_j$ :

% $$
% \frac{\partial \ln P_j}{\partial \beta}=\frac{1}{\lambda_k}A_j x_j +
% (\lambda_k-1)\frac{1}{N_k}\sum_{j \in B_k}\frac{1}{\lambda_k}A_jx_j +
% \ln N_k - \frac{1}{\sum_k N_k^{\lambda_k}} \lambda_k N_k ^{\lambda_k -
%   1}\sum_{j \in B_k}\frac{1}{\lambda_k A_j x_j
%   $$


\section{The general extreme value model}


\cite{MCFAD:78} developed a general model that suppose that the join
distribution of the error terms follow a a multivariate extreme value
distribution. Let $G$ be a function with $J$ arguments $y_j$. $G$ has
the following characteristics :

\begin{itemize}
\item all of its arguments are non-negative,
\item it is non negative,
\item it is homogeneous of degree 1 in all its arguments,
\item for all its argument, $\lim_{y_j \rightarrow
    +\infty}=G(y_1,\ldots y_J)=+\infty$,
\item for distinct arguments, $\frac{\partial^k G}{\partial y_i,
    ...,y_j}$ is non-negative if $k$ is odd and non-positive if $k$
  is even.
\end{itemize}

Assume now that the joint cumulative distribution of the error terms
can be written :

$$
F(\epsilon_1, \epsilon_2, \ldots, \epsilon_J) =
\mbox{exp}\left(-G\left(e^{-\epsilon_1}, e^{-\epsilon_2}, \ldots,
    e^{-\epsilon_J}\right)\right)
$$

We first show that this is a multivariate extreme value
distribution. This implies :

\begin{enumerate}
\item if $F$ is a joint cumulative distribution of probability, for
  any $\epsilon \Rightarrow -\infty$, we should have $F \Rightarrow
  0$,
\item if $F$ is a joint cumulative distribution of probability, for
  all $\epsilon \rightarrow +\infty$, we should have $F \rightarrow
  1$,
\item if $F$ is a multivariate extreme value distribution, the
  marginal distribution of any $\epsilon$ should be an extreme value
  distribution.
\end{enumerate}


For point 1, if $\epsilon_j \rightarrow -\infty$, $y_j \rightarrow
+\infty$, $G \rightarrow +\infty$ and then $F \rightarrow 0$.
  
For point 2, if $(\epsilon_1, \ldots, \epsilon_J) \rightarrow
+\infty$, $G \rightarrow 0$ and then $F \rightarrow 1$.

To demonstrate the third point, we compute the marginal cumulative
distribution of $\epsilon_l$ which is :

$$
F(\epsilon_l) = \lim_{\epsilon_j \rightarrow +\infty \forall j \neq l}
F(\epsilon_1, \ldots, \epsilon_l, \ldots \epsilon_J)
=
$$

$$
F(\epsilon_l) = \mbox{exp}\left(-G\left(0, \ldots, e^{-\epsilon_l}, \ldots, 0\right)\right)
$$

with $G$ being homogeneous of degree one, we have :

$$
G\left(0, \ldots, e^{-\epsilon_l}, \ldots, 0\right) = a_l e^{-\epsilon_l}
$$

with $a_l = G(0, \ldots, 1, \ldots, 0)$. The marginal distribution of
$\epsilon_l$ is then :

$$
F(\epsilon_l) = \mbox{exp}\left(-a_l e^{-\epsilon_l}\right)
$$

which is an uni-variate extreme value distribution. 


We note compute the probabilities of choosing an alternative :


We denote $G_l$ the derivative of $G$ respective to the $l^{\mbox{th}}$
argument. The derivative of $F$ respective to the $\epsilon_l$ is then :

$$
F_l(\epsilon_1, \epsilon_2, \ldots, \epsilon_J) =
e^{-\epsilon_l}G_l\left(e^{-\epsilon_1}, e^{-\epsilon_2}, \ldots,
  e^{-\epsilon_J}\right)\mbox{exp}\left(-G\left(e^{-\epsilon_1},
    e^{-\epsilon_2}, \ldots, e^{-\epsilon_J}\right)\right)
$$

which is the density of $\epsilon_l$ for given values of the other
$J-1$ error terms.

The probability of choosing alternative $l$ is the probability that
$U_l > U_j \; \forall j \neq l$ which is equivalent to $\epsilon_j <
V_l - V_j + \epsilon_l$.

This probability is then : 

$$
\begin{array}{rcl}
  P_l &=& \int_{-\infty}^{+\infty}F_l(V_l-V_1+\epsilon_l, V_l-V_2+\epsilon_l, \ldots, V_l-V_J+\epsilon_l) d\epsilon_l\\
  &=& \int_{-\infty}^{+\infty}e^{-\epsilon_l}G_l\left(e^{-V_l+V_1-\epsilon_l}, e^{-V_l+V_2-\epsilon_l}, \ldots, e^{-V_l+V_J-\epsilon_l}\right)\\
  &\times& \mbox{exp}\left(-G\left(e^{-V_l+V_1-\epsilon_l}, e^{-V_l+V_2-\epsilon_l}, \ldots, e^{-V_l+V_J-\epsilon_l}\right)\right) d\epsilon_l
\end{array}
$$


$G$ being homogeneous of degree one, one can write :

$$
G\left(e^{-V_l+V_1-\epsilon_l}, e^{-V_l+V_2-\epsilon_l}, \ldots,
  e^{-V_l+V_J-\epsilon_l}\right) = e^{-V_l}e^{-\epsilon_l} \times
G\left(e^{V_1}, e^{V_2}, \ldots, e^{V_J}\right)
$$

Homogeneity of degree one implies homogeneity of degree 0 of the first
derivative :


$$
G_l\left(e^{-V_l+V_1-\epsilon_l}, e^{-V_l+V_2-\epsilon_l}, \ldots,
  e^{-V_l-V_J-\epsilon_l}\right) = G_l\left(e^{V_1}, e^{V_2}, \ldots,
  e^{V_J}\right)
$$

The probability of choosing alternative $i$ is then :

$$
P_l = \int_{-\infty}^{+\infty}e^{-\epsilon_l} G_l\left(e^{V_1},
  e^{V_2}, \ldots, e^{V_J}\right)
\mbox{exp}\left(-e^{-\epsilon_l}e^{-V_l}G\left(e^{V_1}, e^{V_2},
    \ldots, e^{V_J}\right)\right) d\epsilon_l
$$

$$
P_l=G_l
\int_{-\infty}^{+\infty}e^{-\epsilon_l}\mbox{exp}\left(-e^{-\epsilon_l}e^{-V_l}G\right)
d\epsilon_l
$$

$$
P_l=G_l
\frac{1}{e^{-V_l}G}\left[\mbox{exp}\left(-e^{-\epsilon_l}e^{-V_l}G\right)\right]_{-\infty}^{+\infty}
= \frac{G_l}{e^{-V_l}G}
$$

Finally, the probability of choosing alternative $i$ can be written :

$$
P_l = \frac{e^{V_l}G_l\left(e^{V_1}, e^{V_2}, \ldots,
    e^{V_J}\right)}{G\left(e^{V_1}, e^{V_2}, \ldots, e^{V_J}\right)}
$$




\section{The random parameters (or mixed) logit model}

A mixed logit model or random parameters logit model is a logit model
for which the parameters are assumed to vary from one individual to
another.

\subsection{The probabilities}

The standard logit model is :
$$
P_{il}=\frac{e^{\beta'x_{il}}}{\sum_j e^{\beta'x_{ij}}}
$$

The mixed logit model is :
$$
P_{il}=\frac{e^{\beta_i'x_{il}}}{\sum_j e^{\beta_i'x_{ij}}}
$$

Two strategies of estimation may be considered :

\begin{itemize}
\item estimate the coefficients for each individual in the sample,
\item consider the coefficients as random variables.
\end{itemize}

The first approach is of limited interest, because it would requires
numerous observations for each individual.

The second approach leads to the mixed logit model.

The probability that individual $i$ will choose alternative $l$ is :

$$
P_{il} \mid \beta_i =\frac{e^{\beta_i'x_{il}}}{\sum_j e^{\beta_i'x_{ij}}}
$$

This is the probability for individual $i$ conditional on the vector
of coefficients $\beta_i$. To get the unconditional probability, we
have the average probability for the different values of $\beta_i$.

If $V_{il}=\alpha_i+\beta_i x_{il}$ and the density of $\beta_i$ is
$f(\beta_i,\theta)$ :

$$
P_{il}= \mbox{E}(P_{il} \mid \beta_i) =
\int_{-\infty}^{+\infty}(P_{il} \mid \beta_i)f(\beta_i,\theta)d\beta_i
$$

which can be estimated efficiently by quadrature methods.

If $V_{il}=\alpha_i+\beta_i x_{il}+\gamma_i v_{il}$ and the density of
$\beta_i$ and $\gamma_i$ is $f(\beta_i,\gamma_i,\theta)$

$$
P_{il}= \mbox{E}(P_{il} \mid \beta_i, \gamma_i) =
\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}(P_{il} \mid \beta_i,
\gamma_i)f(\beta_i,\gamma_i\theta)d\beta_i d\gamma_i
$$

can be estimated by simulations.

\subsection{Panel data}

Especially important for stated preference survey where several
questions are asked to every individual.

Joint probabilities for each individual are computed.

$$P_{ikl}^r =\frac{e^{\beta_i^rx_{ikl}}}{\sum_j e^{\beta_i^rx_{ikj}}}$$

$$P_{ik}^r=\prod_l {P_{ikl}^r}^{y_{ikl}}$$

$$P_{i}^r=\prod_i \prod_l {P_{ikl}^r}^{y_{ikl}}$$

$$
\bar{P}_i=\frac{1}{R}P_{i}^r
$$

\subsection{Simulations}

The probabilities for the random parameter logit are integrals with no
closed form. Moreover, the degree of integration is the number of
random parameters. In practice, these models are estimated using
simulation techniques, \emph{i.e.} the expected value is replaced by
an arithmetic mean. More precisely :

\begin{itemize}
\item make an initial hypothesis about the distribution of the random
  parameter : $\beta_i$ follows a normal distribution with mean $\mu$
  and standard deviation $\sigma$,
\item draw $R$ numbers on this distribution,
\item for each draw $\beta_i^r$, compute the probability :
  $P_{il}^r =\frac{e^{\beta_i^rx_{il}}}{\sum_j e^{\beta_i^rx_{ij}}}$
\item compute the average of these probabilities :
  $\bar{\mbox{P}}_{il}=\sum_{r=1}^n\mbox{P}_{il}/R$ 
\item compute the log--likelihood for these probabilities,
\item iterate until the maximum.
\end{itemize}



\subsubsection{Drawing from densities}

\begin{itemize}
\item use \texttt{runif} to generate pseudo random-draws from a
  uniform distribution,
\item transform this random numbers with the quantile function of the
  required distribution.
\end{itemize}

ex: for the Gumbell distribution :

$$
F(x)=e^{-e^{-x}}
\Rightarrow F^{-1}(x)=-\ln(-\ln(x))
$$

Problem : not good coverage of the relevant interval instead numerous
draws are made. More deterministic methods like Halton draws may be
used instead.

\subsubsection{Halton sequence}

To generate a Halton sequence, use a prime (e.g. 3). The sequence is
then :

\begin{itemize}
\item 0 --- 1/3 --- 2/3,
\item 0+1/9 --- 1/3+1/9 --- 2/3+1/9 --- 0+2/9 --- 1/3+2/9  --- 2/3+2/9,
\item 0+1/27 --- 1/3++1/27 --- 2/3+1/9+1/27 --- 1/3+2/9+1/27 --- 2/3+2/9+1/27
  --- 1/3+1/9+2/27 --- 2/3+1/9+2/27 --- 1/3+2/9+2/27 --- 2/3+2/9+2/27
\end{itemize}

\begin{center}
  \begin{figure}
\caption{Halton sequences}
\includegraphics[width=\textwidth]{./graph/halton3.pdf}
  \end{figure}
\end{center}

\begin{center}
  \begin{figure}
\caption{Halton sequences vs random numbers in two dimensions}
\includegraphics[width=\textwidth]{./graph/haltonvsrandom.pdf}
  \end{figure}
\end{center}

\begin{center}
  \begin{figure}
\caption{uniform to Gumbell deviates}
\includegraphics[width=0.7\textwidth]{./graph/draws.pdf}
  \end{figure}
\end{center}


\subsubsection{Correlation}

Cholesky decomposition is used :

$
\Omega
$ is the covariance matrix of two random parameters. 

The Cholesky matrix is :
$$
C=
\left(
  \begin{array}{cc}
    c_{11} & c_{12} \\
    0 & c_{22}
  \end{array}
\right)
$$

so that 

$$
C^{\top} C = 
\left(
\begin{array}{cc}
c_{11}^2 & c_{11} c_{12} \\
c_{11} c_{12} & c_{12}^2+c_{22}^2
\end{array}
\right)
=
\Omega
$$


if $\mbox{V}(\epsilon_1,\epsilon_2)=I$, then the variance of 
$
(\epsilon_1 \epsilon_2) C
$ 
is $\Omega$

ex : 
$$
\Omega=
\left(
  \begin{array}{cc}
    0.5 & 0.8 \\
    0.8 & 2.0
  \end{array}
\right)
\mbox{  and  }
C=
\left(
  \begin{array}{cc}
    0.71 & 1.13 \\
    0 & 0.85
  \end{array}
\right)
$$

$$
\left\{
  \begin{array}{rcl}
    \beta_1 &=& 0.71 \epsilon_1 \\
    \beta_2 &=& 1.13 \epsilon_1+0.85 \epsilon_2
  \end{array}
\right.
$$
\begin{center}
  \begin{figure}
\includegraphics[width=\textwidth]{./graph/correlation.pdf}
\caption{Correlation}
\end{figure}
\end{center}


\bibliography{bibmlogit}

\end{document}



